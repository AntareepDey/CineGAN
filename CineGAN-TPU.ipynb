{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two cells are required for now to run in Kaggle v5e-8 TPU as the installed tensorflow verison does not support TPU. In th future these two cells can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T05:00:58.691919Z",
     "iopub.status.busy": "2025-11-21T05:00:58.691648Z",
     "iopub.status.idle": "2025-11-21T05:00:58.834921Z",
     "shell.execute_reply": "2025-11-21T05:00:58.833991Z",
     "shell.execute_reply.started": "2025-11-21T05:00:58.691898Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!export PATH=\"${HOME}/.local/bin:${PATH}\" && uv pip uninstall --system jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T05:01:01.088175Z",
     "iopub.status.busy": "2025-11-21T05:01:01.087915Z",
     "iopub.status.idle": "2025-11-21T05:01:01.213298Z",
     "shell.execute_reply": "2025-11-21T05:01:01.212420Z",
     "shell.execute_reply.started": "2025-11-21T05:01:01.088152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!export PATH=\"${HOME}/.local/bin:${PATH}\" && uv pip install --system tensorflow-tpu==\"2.18.0\" --find-links https://storage.googleapis.com/libtpu-tf-releases/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code begine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-21T05:01:03.950205Z",
     "iopub.status.busy": "2025-11-21T05:01:03.949950Z",
     "iopub.status.idle": "2025-11-21T05:01:07.673003Z",
     "shell.execute_reply": "2025-11-21T05:01:07.671892Z",
     "shell.execute_reply.started": "2025-11-21T05:01:03.950183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import mixed_precision\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T05:01:11.434123Z",
     "iopub.status.busy": "2025-11-21T05:01:11.433573Z",
     "iopub.status.idle": "2025-11-21T05:01:28.365905Z",
     "shell.execute_reply": "2025-11-21T05:01:28.364843Z",
     "shell.execute_reply.started": "2025-11-21T05:01:11.434096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TPU Setup\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
    "    print('Device:', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "print(tf.__version__)\n",
    "\n",
    "with strategy.scope():\n",
    "    policy_type = 'mixed_bfloat16' if isinstance(strategy, tf.distribute.TPUStrategy) else 'mixed_float16'\n",
    "    mixed_precision.set_global_policy(policy_type)\n",
    "    print(f\"Mixed Precision Policy: {policy_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T05:03:15.843188Z",
     "iopub.status.busy": "2025-11-21T05:03:15.842957Z",
     "iopub.status.idle": "2025-11-21T05:03:16.166685Z",
     "shell.execute_reply": "2025-11-21T05:03:16.165632Z",
     "shell.execute_reply.started": "2025-11-21T05:03:15.843172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 8* strategy.num_replicas_in_sync\n",
    "EPOCHS = 50\n",
    "LAMBDA_GAN = 1.0\n",
    "LAMBDA_FM = 10.0\n",
    "LAMBDA_VGG = 2\n",
    "GEN_LR = 0.00005\n",
    "DISC_LR = 0.0001\n",
    "\n",
    "# Load dataset paths\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path()\n",
    "cinematic_train = tf.io.gfile.glob(str(GCS_PATH + '/FilmSet/train/ClassNeg/*.png'))\n",
    "input_train = tf.io.gfile.glob(str(GCS_PATH + '/FilmSet/train/input/*.png'))\n",
    "cinematic_test = tf.io.gfile.glob(str(GCS_PATH + '/FilmSet/test/ClassNeg/*.png'))\n",
    "input_test = tf.io.gfile.glob(str(GCS_PATH + '/FilmSet/test/input/*.png'))\n",
    "\n",
    "print(f'Training - Cinematic images: {len(cinematic_train)}')\n",
    "print(f'Training - Input images: {len(input_train)}')\n",
    "print(f'Test - Cinematic images: {len(cinematic_test)}')\n",
    "print(f'Test - Input images: {len(input_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T05:03:21.833227Z",
     "iopub.status.busy": "2025-11-21T05:03:21.832964Z",
     "iopub.status.idle": "2025-11-21T05:03:21.846717Z",
     "shell.execute_reply": "2025-11-21T05:03:21.845834Z",
     "shell.execute_reply.started": "2025-11-21T05:03:21.833211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check that all image pairs exist\n",
    "def verify_pairs(input_paths, target_paths):\n",
    "    input_names = {os.path.basename(path): path for path in input_paths}\n",
    "    target_names = {os.path.basename(path): path for path in target_paths}\n",
    "    \n",
    "    missing_in_target = set(input_names.keys()) - set(target_names.keys())\n",
    "    missing_in_input = set(target_names.keys()) - set(input_names.keys())\n",
    "    \n",
    "    if missing_in_target:\n",
    "        print(f\"Warning: {len(missing_in_target)} files missing in target folder\")\n",
    "    if missing_in_input:\n",
    "        print(f\"Warning: {len(missing_in_input)} files missing in input folder\")\n",
    "    \n",
    "    common = set(input_names.keys()) & set(target_names.keys())\n",
    "    paired_inputs = [input_names[name] for name in sorted(common)]\n",
    "    paired_targets = [target_names[name] for name in sorted(common)]\n",
    "    \n",
    "    return paired_inputs, paired_targets\n",
    "\n",
    "input_train, cinematic_train = verify_pairs(input_train, cinematic_train)\n",
    "input_test, cinematic_test = verify_pairs(input_test, cinematic_test)\n",
    "print(f'Verified paired training samples: {len(input_train)}')\n",
    "print(f'Verified paired test samples: {len(input_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T05:03:25.930323Z",
     "iopub.status.busy": "2025-11-21T05:03:25.930093Z",
     "iopub.status.idle": "2025-11-21T05:03:44.891956Z",
     "shell.execute_reply": "2025-11-21T05:03:44.890774Z",
     "shell.execute_reply.started": "2025-11-21T05:03:25.930306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    img = (img / 127.5) - 1.0  # Normalize to [-1, 1]\n",
    "    return img\n",
    "\n",
    "def load_paired_images(input_path, target_path):\n",
    "    input_img = load_image(input_path)\n",
    "    target_img = load_image(target_path)\n",
    "    return input_img, target_img\n",
    "\n",
    "# Stateless augmentation for determinism\n",
    "def augment(input_img, target_img):\n",
    "    seed = tf.random.uniform(shape=[2], minval=0, maxval=2**31 - 1, dtype=tf.int32)\n",
    "    input_img = tf.image.stateless_random_flip_left_right(input_img, seed)\n",
    "    target_img = tf.image.stateless_random_flip_left_right(target_img, seed)\n",
    "    return input_img, target_img\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((input_train, cinematic_train))\n",
    "train_dataset = train_dataset.shuffle(1000)\n",
    "train_dataset = train_dataset.map(load_paired_images, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.cache()  # Cache decoded images in RAM\n",
    "train_dataset = train_dataset.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.prefetch(AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((input_test, cinematic_test))\n",
    "test_dataset = test_dataset.map(load_paired_images, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache()\n",
    "test_dataset = test_dataset.batch(1).prefetch(AUTOTUNE)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
    "\n",
    "# Custom Layers\n",
    "class SEBlock(layers.Layer):\n",
    "    def __init__(self, filters, ratio=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.ratio = ratio\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.dense1 = layers.Dense(self.filters // self.ratio, activation='relu')\n",
    "        self.dense2 = layers.Dense(self.filters, activation='sigmoid')\n",
    "        self.reshape = layers.Reshape((1, 1, self.filters))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        se = self.global_pool(inputs)\n",
    "        se = self.dense1(se)\n",
    "        se = self.dense2(se)\n",
    "        se = self.reshape(se)\n",
    "        return inputs * se\n",
    "\n",
    "class ResidualBlock(layers.Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.conv1 = layers.Conv2D(self.filters, 3, padding='same')\n",
    "        self.norm1 = tf.keras.layers.GroupNormalization(groups=-1)\n",
    "        self.conv2 = layers.Conv2D(self.filters, 3, padding='same')\n",
    "        self.norm2 = tf.keras.layers.GroupNormalization(groups=-1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.norm1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        return inputs + x\n",
    "\n",
    "def pixel_shuffle(x, scale=2):\n",
    "    return tf.nn.depth_to_space(x, scale)\n",
    "\n",
    "# Generator: \n",
    "def build_generator():\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    # Encoder\n",
    "    e1 = layers.Conv2D(64, 7, padding='same')(inputs)\n",
    "    e1 = tf.keras.layers.GroupNormalization(groups=-1)(e1)\n",
    "    e1 = layers.ReLU()(e1)\n",
    "    \n",
    "    e2 = tf.keras.layers.SpectralNormalization(layers.Conv2D(128, 3, strides=2, padding='same'))(e1)\n",
    "    e2 = tf.keras.layers.GroupNormalization(groups=-1)(e2)\n",
    "    e2 = layers.ReLU()(e2)\n",
    "    \n",
    "    e3 = tf.keras.layers.SpectralNormalization(layers.Conv2D(256, 3, strides=2, padding='same'))(e2)\n",
    "    e3 = tf.keras.layers.GroupNormalization(groups=-1)(e3)\n",
    "    e3 = layers.ReLU()(e3)\n",
    "\n",
    "    e4 = tf.keras.layers.SpectralNormalization(layers.Conv2D(512, 3, strides=2, padding='same'))(e3)\n",
    "    e4 = tf.keras.layers.GroupNormalization(groups=-1)(e4)\n",
    "    e4 = layers.ReLU()(e4)\n",
    "    x = e4\n",
    "    for _ in range(9):\n",
    "        x = ResidualBlock(512)(x)\n",
    "    \n",
    "    # Decoder with attention skip connections\n",
    "    # Upsample 1\n",
    "    x = layers.Conv2D(1024, 3, padding='same')(x)\n",
    "    x = layers.Lambda(lambda t: pixel_shuffle(t, 2))(x)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    e3_att = SEBlock(256)(e3)\n",
    "    x = layers.Concatenate()([x, e3_att])\n",
    "    x = layers.Conv2D(256, 3, padding='same')(x)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    # Upsample 2\n",
    "    x = layers.Conv2D(512, 3, padding='same')(x)\n",
    "    x = layers.Lambda(lambda t: pixel_shuffle(t, 2))(x)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    e2_att = SEBlock(128)(e2)\n",
    "    x = layers.Concatenate()([x, e2_att])\n",
    "    x = layers.Conv2D(128, 3, padding='same')(x)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    # Upsample 3\n",
    "    x = layers.Conv2D(256, 3, padding='same')(x)\n",
    "    x = layers.Lambda(lambda t: pixel_shuffle(t, 2))(x)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    e1_att = SEBlock(64)(e1)\n",
    "    x = layers.Concatenate()([x, e1_att])\n",
    "    x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Conv2D(3, 7, padding='same', activation='tanh')(x)\n",
    "\n",
    "    \n",
    "    return keras.Model(inputs, outputs, name='generator')\n",
    "\n",
    "# Discriminator: Multi-Scale Spectral PatchGAN\n",
    "def build_discriminator(name='discriminator'):\n",
    "    inputs = layers.Input(shape=(None, None, 6))\n",
    "    \n",
    "    x = tf.keras.layers.SpectralNormalization(layers.Conv2D(64, 4, strides=2, padding='same'))(inputs)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.SpectralNormalization(layers.Conv2D(128, 4, strides=2, padding='same'))(x)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1)(x)\n",
    "    f1 = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.SpectralNormalization(layers.Conv2D(256, 4, strides=2, padding='same'))(f1)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1)(x)\n",
    "    f2 = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.SpectralNormalization(layers.Conv2D(512, 4, strides=1, padding='same'))(f2)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1)(x)\n",
    "    f3 = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.SpectralNormalization(layers.Conv2D(1, 4, strides=1, padding='same'))(f3)\n",
    "    \n",
    "    return keras.Model(inputs, [x, f1, f2, f3], name=name)\n",
    "\n",
    "# VGG for Perceptual Loss with float32 output\n",
    "def build_vgg():\n",
    "    vgg = keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "    layer_name = 'block4_conv2'\n",
    "    outputs = vgg.get_layer(layer_name).output\n",
    "    outputs = layers.Activation('linear', dtype='float32')(outputs)\n",
    "    return keras.Model(vgg.input, outputs, name='vgg_features')\n",
    "\n",
    "# Loss Functions\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_output = tf.cast(real_output, tf.float32)\n",
    "    fake_output = tf.cast(fake_output, tf.float32)\n",
    "    real_loss = tf.reduce_mean(tf.maximum(0.0, 1.0 - real_output))\n",
    "    fake_loss = tf.reduce_mean(tf.maximum(0.0, 1.0 + fake_output))\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    fake_output = tf.cast(fake_output, tf.float32)\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "def feature_matching_loss(real_features, fake_features):\n",
    "    loss = 0\n",
    "    for real_f, fake_f in zip(real_features, fake_features):\n",
    "        real_f = tf.cast(real_f, tf.float32)\n",
    "        fake_f = tf.cast(fake_f, tf.float32)\n",
    "        loss += tf.reduce_mean(tf.abs(real_f - fake_f))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def perceptual_loss(vgg_model, real_img, fake_img):\n",
    "    # Combine real and fake for single forward pass\n",
    "    real_img = tf.cast(real_img, tf.float32)\n",
    "    fake_img = tf.cast(fake_img, tf.float32)\n",
    "    combined = tf.concat([real_img, fake_img], axis=0)\n",
    "    \n",
    "    # Convert from [-1, 1] to [0, 255]\n",
    "    combined = (combined + 1) * 127.5\n",
    "    \n",
    "    # Apply proper VGG preprocessing (RGB to BGR + ImageNet mean subtraction)\n",
    "    combined = tf.keras.applications.vgg19.preprocess_input(combined)\n",
    "    \n",
    "    # Single VGG forward pass\n",
    "    features = vgg_model(combined)\n",
    "    \n",
    "    # Split features back\n",
    "    batch_size = tf.shape(real_img)[0]\n",
    "    real_features = features[:batch_size]\n",
    "    fake_features = features[batch_size:]\n",
    "    \n",
    "    return tf.reduce_mean(tf.abs(real_features - fake_features))\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_image, target_image, generator, disc1, disc2, vgg, \n",
    "               gen_optimizer, disc_optimizer):\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generate fake images\n",
    "        fake_image = generator(input_image, training=True)\n",
    "        fake_image = tf.cast(fake_image, tf.float32)\n",
    "        # Discriminator 1 (full resolution)\n",
    "        real_pair1 = tf.concat([input_image, target_image], axis=-1)\n",
    "        fake_pair1 = tf.concat([input_image, fake_image], axis=-1)\n",
    "        disc1_real, *disc1_real_features = disc1(real_pair1, training=True)\n",
    "        disc1_fake, *disc1_fake_features = disc1(fake_pair1, training=True)\n",
    "        \n",
    "        # Discriminator 2 (half resolution) - Batch resize\n",
    "        combined_images = tf.concat([input_image, target_image, fake_image], axis=0)\n",
    "        combined_half = tf.image.resize(combined_images, [IMG_SIZE // 2, IMG_SIZE // 2])\n",
    "        input_half, target_half, fake_half = tf.split(combined_half, 3, axis=0)\n",
    "        \n",
    "        real_pair2 = tf.concat([input_half, target_half], axis=-1)\n",
    "        fake_pair2 = tf.concat([input_half, fake_half], axis=-1)\n",
    "        disc2_real, *disc2_real_features = disc2(real_pair2, training=True)\n",
    "        disc2_fake, *disc2_fake_features = disc2(fake_pair2, training=True)\n",
    "        \n",
    "        # Discriminator losses\n",
    "        disc1_loss = discriminator_loss(disc1_real, disc1_fake)\n",
    "        disc2_loss = discriminator_loss(disc2_real, disc2_fake)\n",
    "        total_disc_loss = disc1_loss + disc2_loss\n",
    "        \n",
    "        # Generator losses\n",
    "        gen_gan_loss = (generator_loss(disc1_fake) + generator_loss(disc2_fake)) / 2\n",
    "        gen_fm_loss = (feature_matching_loss(disc1_real_features, disc1_fake_features) +\n",
    "                       feature_matching_loss(disc2_real_features, disc2_fake_features)) / 2\n",
    "        \n",
    "        # VGG perceptual loss with batched forward pass\n",
    "        gen_vgg_loss = perceptual_loss(vgg, target_image, fake_image)\n",
    "        \n",
    "        total_gen_loss = (LAMBDA_GAN * gen_gan_loss + \n",
    "                         LAMBDA_FM * gen_fm_loss + \n",
    "                         LAMBDA_VGG * gen_vgg_loss)\n",
    "    \n",
    "    gen_gradients = gen_tape.gradient(total_gen_loss, generator.trainable_variables)\n",
    "    disc_gradients = disc_tape.gradient(total_disc_loss, \n",
    "                                        disc1.trainable_variables + disc2.trainable_variables)\n",
    "    \n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(disc_gradients, \n",
    "                                       disc1.trainable_variables + disc2.trainable_variables))\n",
    "    \n",
    "    return total_gen_loss, total_disc_loss, gen_gan_loss, gen_fm_loss, gen_vgg_loss\n",
    "\n",
    "# Distributed training step\n",
    "@tf.function\n",
    "def distributed_train_step(input_image, target_image):\n",
    "    def train_step_wrapper(input_img, target_img):\n",
    "        return train_step(input_img, target_img, generator, discriminator1, \n",
    "                         discriminator2, vgg_model, gen_optimizer, disc_optimizer)\n",
    "    \n",
    "    per_replica_losses = strategy.run(train_step_wrapper, args=(input_image, target_image))\n",
    "    \n",
    "    # Reduce and average losses\n",
    "    num_replicas = tf.cast(strategy.num_replicas_in_sync, tf.float32)\n",
    "    \n",
    "    total_gen_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[0], axis=None) / num_replicas\n",
    "    total_disc_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[1], axis=None) / num_replicas\n",
    "    gen_gan_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[2], axis=None) / num_replicas\n",
    "    gen_fm_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[3], axis=None) / num_replicas\n",
    "    gen_vgg_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[4], axis=None) / num_replicas\n",
    "    \n",
    "    return total_gen_loss, total_disc_loss, gen_gan_loss, gen_fm_loss, gen_vgg_loss\n",
    "\n",
    "# Visualization function\n",
    "def generate_and_save_images(generator, test_input, target, epoch, num_examples=3):\n",
    "    predictions = generator(test_input, training=False)\n",
    "    predictions = predictions.numpy().astype(np.float32)\n",
    "    test_input = test_input.numpy().astype(np.float32)\n",
    "    target = target.numpy().astype(np.float32)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5 * num_examples))\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        # Input\n",
    "        plt.subplot(num_examples, 3, i * 3 + 1)\n",
    "        plt.imshow(test_input[i] * 0.5 + 0.5)\n",
    "        plt.title('Input')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Generated\n",
    "        plt.subplot(num_examples, 3, i * 3 + 2)\n",
    "        plt.imshow(predictions[i] * 0.5 + 0.5)\n",
    "        plt.title('Generated')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Target\n",
    "        plt.subplot(num_examples, 3, i * 3 + 3)\n",
    "        plt.imshow(target[i] * 0.5 + 0.5)\n",
    "        plt.title('Target (Cinematic)')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Epoch {epoch}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'generated_epoch_{epoch}.png')\n",
    "    plt.close()\n",
    "\n",
    "with strategy.scope():\n",
    "    generator = build_generator()\n",
    "    discriminator1 = build_discriminator(name='discriminator_full')\n",
    "    discriminator2 = build_discriminator(name='discriminator_half')\n",
    "    vgg_model = build_vgg()\n",
    "    \n",
    "    # Optimizers with mixed precision wrapper\n",
    "    gen_optimizer = keras.optimizers.Adam(learning_rate=GEN_LR, beta_1=0.0, beta_2=0.9)\n",
    "    disc_optimizer = keras.optimizers.Adam(learning_rate=DISC_LR, beta_1=0.0, beta_2=0.9)\n",
    "    \n",
    "    gen_optimizer = mixed_precision.LossScaleOptimizer(gen_optimizer)\n",
    "    disc_optimizer = mixed_precision.LossScaleOptimizer(disc_optimizer)\n",
    "    \n",
    "    # Metrics\n",
    "    gen_loss_metric = keras.metrics.Mean(name='gen_loss', dtype=tf.float32)\n",
    "    disc_loss_metric = keras.metrics.Mean(name='disc_loss', dtype=tf.float32)\n",
    "    gan_loss_metric = keras.metrics.Mean(name='gan_loss', dtype=tf.float32)\n",
    "    fm_loss_metric = keras.metrics.Mean(name='fm_loss', dtype=tf.float32)\n",
    "    vgg_loss_metric = keras.metrics.Mean(name='vgg_loss', dtype=tf.float32)\n",
    "    \n",
    "    print(\"Generator parameters:\", generator.count_params())\n",
    "    print(\"Discriminator 1 parameters:\", discriminator1.count_params())\n",
    "    print(\"Discriminator 2 parameters:\", discriminator2.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T05:03:48.216465Z",
     "iopub.status.busy": "2025-11-21T05:03:48.216222Z",
     "iopub.status.idle": "2025-11-21T05:03:48.290687Z",
     "shell.execute_reply": "2025-11-21T05:03:48.289495Z",
     "shell.execute_reply.started": "2025-11-21T05:03:48.216449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare test samples for visualization\n",
    "test_samples = list(test_dataset.take(3))\n",
    "test_inputs = tf.concat([sample[0] for sample in test_samples], axis=0)\n",
    "test_targets = tf.concat([sample[1] for sample in test_samples], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T05:03:53.989725Z",
     "iopub.status.busy": "2025-11-21T05:03:53.989437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nStarting Optimized Training...\")\n",
    "train_dataset_dist = strategy.experimental_distribute_dataset(train_dataset)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Reset metrics at the start of each epoch\n",
    "    gen_loss_metric.reset_state()\n",
    "    disc_loss_metric.reset_state()\n",
    "    gan_loss_metric.reset_state()\n",
    "    fm_loss_metric.reset_state()\n",
    "    vgg_loss_metric.reset_state()\n",
    "    \n",
    "    # Progress bar for the epoch\n",
    "    pbar = tqdm(enumerate(train_dataset_dist), total=steps_per_epoch, desc=f'Epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (input_img, target_img) in pbar:\n",
    "        gen_loss, disc_loss, gan_loss, fm_loss, vgg_loss = distributed_train_step(input_img, target_img)\n",
    "        \n",
    "        # Update metrics\n",
    "        gen_loss_metric.update_state(gen_loss)\n",
    "        disc_loss_metric.update_state(disc_loss)\n",
    "        gan_loss_metric.update_state(gan_loss)\n",
    "        fm_loss_metric.update_state(fm_loss)\n",
    "        vgg_loss_metric.update_state(vgg_loss)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'G_Loss': f'{gen_loss_metric.result():.4f}',\n",
    "            'D_Loss': f'{disc_loss_metric.result():.4f}',\n",
    "            'GAN': f'{gan_loss_metric.result():.4f}',\n",
    "            'FM': f'{fm_loss_metric.result():.4f}',\n",
    "            'VGG': f'{vgg_loss_metric.result():.4f}'\n",
    "        })\n",
    "    \n",
    "    # Epoch summary\n",
    "    print(f'\\nEpoch {epoch + 1} Summary:')\n",
    "    print(f'  Avg Gen Loss:  {gen_loss_metric.result():.4f}')\n",
    "    print(f'  Avg Disc Loss: {disc_loss_metric.result():.4f}')\n",
    "    print(f'  Avg GAN Loss:  {gan_loss_metric.result():.4f}')\n",
    "    print(f'  Avg FM Loss:   {fm_loss_metric.result():.4f}')\n",
    "    print(f'  Avg VGG Loss:  {vgg_loss_metric.result():.4f}')\n",
    "    \n",
    "    # Generate and save sample images every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        generate_and_save_images(generator, test_inputs, test_targets, epoch + 1)\n",
    "        print(f'Saved sample images for epoch {epoch + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Saving models for fine-tuning...\")\n",
    "model_dir = '/kaggle/working/cinematic_gan_model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "generator_path = os.path.join(model_dir, 'generator.keras')\n",
    "generator.save(generator_path)\n",
    "print(f'Generator saved to: {generator_path}')\n",
    "discriminator1_path = os.path.join(model_dir, 'discriminator1.keras')\n",
    "discriminator1.save(discriminator1_path)\n",
    "print(f'Discriminator 1 saved to: {discriminator1_path}')\n",
    "\n",
    "discriminator2_path = os.path.join(model_dir, 'discriminator2.keras')\n",
    "discriminator2.save(discriminator2_path)\n",
    "print(f'Discriminator 2 saved to: {discriminator2_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning to remove artifacts if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def total_variation_loss(image):\n",
    "\n",
    "    # Cast to float32 for computation\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    # Horizontal differences\n",
    "    x_diff = tf.reduce_mean(tf.abs(image[:, :, :-1, :] - image[:, :, 1:, :]))\n",
    "    # Vertical differences\n",
    "    y_diff = tf.reduce_mean(tf.abs(image[:, :-1, :, :] - image[:, 1:, :, :]))\n",
    "    \n",
    "    return x_diff + y_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updated hyperparameters for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "LAMBDA_GAN = 1.0\n",
    "LAMBDA_FM = 10.0\n",
    "LAMBDA_VGG = 1.0     \n",
    "LAMBDA_TV = 20.0      \n",
    "GEN_LR = 0.000025      \n",
    "DISC_LR = 0.0001      \n",
    "FINE_TUNE_EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_step(input_image, target_image, generator, disc1, disc2, vgg, \n",
    "               gen_optimizer, disc_optimizer):\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generate fake images\n",
    "        fake_image = generator(input_image, training=True)\n",
    "        fake_image = tf.cast(fake_image, tf.float32)\n",
    "        \n",
    "        # Discriminator 1 (full resolution)\n",
    "        real_pair1 = tf.concat([input_image, target_image], axis=-1)\n",
    "        fake_pair1 = tf.concat([input_image, fake_image], axis=-1)\n",
    "        disc1_real, *disc1_real_features = disc1(real_pair1, training=True)\n",
    "        disc1_fake, *disc1_fake_features = disc1(fake_pair1, training=True)\n",
    "        \n",
    "        # Discriminator 2 (half resolution) - Batch resize\n",
    "        combined_images = tf.concat([input_image, target_image, fake_image], axis=0)\n",
    "        combined_half = tf.image.resize(combined_images, [IMG_SIZE // 2, IMG_SIZE // 2])\n",
    "        input_half, target_half, fake_half = tf.split(combined_half, 3, axis=0)\n",
    "        \n",
    "        real_pair2 = tf.concat([input_half, target_half], axis=-1)\n",
    "        fake_pair2 = tf.concat([input_half, fake_half], axis=-1)\n",
    "        disc2_real, *disc2_real_features = disc2(real_pair2, training=True)\n",
    "        disc2_fake, *disc2_fake_features = disc2(fake_pair2, training=True)\n",
    "        \n",
    "        # Discriminator losses\n",
    "        disc1_loss = discriminator_loss(disc1_real, disc1_fake)\n",
    "        disc2_loss = discriminator_loss(disc2_real, disc2_fake)\n",
    "        total_disc_loss = disc1_loss + disc2_loss\n",
    "        \n",
    "        # Generator losses\n",
    "        gen_gan_loss = (generator_loss(disc1_fake) + generator_loss(disc2_fake)) / 2\n",
    "        gen_fm_loss = (feature_matching_loss(disc1_real_features, disc1_fake_features) +\n",
    "                       feature_matching_loss(disc2_real_features, disc2_fake_features)) / 2\n",
    "        \n",
    "        # VGG perceptual loss\n",
    "        gen_vgg_loss = perceptual_loss(vgg, target_image, fake_image)\n",
    "        gen_tv_loss = total_variation_loss(fake_image)\n",
    "        total_gen_loss = (LAMBDA_GAN * gen_gan_loss + \n",
    "                         LAMBDA_FM * gen_fm_loss + \n",
    "                         LAMBDA_VGG * gen_vgg_loss +\n",
    "                         LAMBDA_TV * gen_tv_loss) \n",
    "    \n",
    "    # Apply gradients\n",
    "    gen_gradients = gen_tape.gradient(total_gen_loss, generator.trainable_variables)\n",
    "    disc_gradients = disc_tape.gradient(total_disc_loss, \n",
    "                                        disc1.trainable_variables + disc2.trainable_variables)\n",
    "    \n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(disc_gradients, \n",
    "                                       disc1.trainable_variables + disc2.trainable_variables))\n",
    "    \n",
    "    return total_gen_loss, total_disc_loss, gen_gan_loss, gen_fm_loss, gen_vgg_loss, gen_tv_loss  # Return TV loss too\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(input_image, target_image):\n",
    "    def train_step_wrapper(input_img, target_img):\n",
    "        return train_step(input_img, target_img, generator, discriminator1, \n",
    "                         discriminator2, vgg_model, gen_optimizer, disc_optimizer)\n",
    "    \n",
    "    per_replica_losses = strategy.run(train_step_wrapper, args=(input_image, target_image))\n",
    "    \n",
    "    num_replicas = tf.cast(strategy.num_replicas_in_sync, tf.float32)\n",
    "    \n",
    "    total_gen_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[0], axis=None) / num_replicas\n",
    "    total_disc_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[1], axis=None) / num_replicas\n",
    "    gen_gan_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[2], axis=None) / num_replicas\n",
    "    gen_fm_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[3], axis=None) / num_replicas\n",
    "    gen_vgg_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[4], axis=None) / num_replicas\n",
    "    gen_tv_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[5], axis=None) / num_replicas  # NEW\n",
    "    \n",
    "    return total_gen_loss, total_disc_loss, gen_gan_loss, gen_fm_loss, gen_vgg_loss, gen_tv_loss  # Return TV\n",
    "\n",
    "def generate_and_save_images(generator, test_input, target, epoch, num_examples=3):\n",
    "    predictions = generator(test_input, training=False)\n",
    "    predictions = predictions.numpy().astype(np.float32)\n",
    "    test_input = test_input.numpy().astype(np.float32)\n",
    "    target = target.numpy().astype(np.float32)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5 * num_examples))\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        # Input\n",
    "        plt.subplot(num_examples, 3, i * 3 + 1)\n",
    "        plt.imshow(test_input[i] * 0.5 + 0.5)\n",
    "        plt.title('Input')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Generated\n",
    "        plt.subplot(num_examples, 3, i * 3 + 2)\n",
    "        plt.imshow(predictions[i] * 0.5 + 0.5)\n",
    "        plt.title('Generated')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Target\n",
    "        plt.subplot(num_examples, 3, i * 3 + 3)\n",
    "        plt.imshow(target[i] * 0.5 + 0.5)\n",
    "        plt.title('Target (Cinematic)')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Epoch {epoch}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Finetune_epoch_{epoch}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    generator = build_generator()\n",
    "    discriminator1 = build_discriminator(name='discriminator_full')\n",
    "    discriminator2 = build_discriminator(name='discriminator_half')\n",
    "    vgg_model = build_vgg()\n",
    "\n",
    "    try:\n",
    "        generator.load_weights('/kaggle/working/cinematic_gan_model/generator.keras')\n",
    "        print(\"Generator weights loaded\")\n",
    "    except:\n",
    "        generator.load_weights('/kaggle/working/cinematic_gan_model/generator')\n",
    "        print(\"Generator weights loaded from SavedModel\")\n",
    "    \n",
    "    try:\n",
    "        discriminator1.load_weights('/kaggle/working/cinematic_gan_model/discriminator1.keras')\n",
    "        discriminator2.load_weights('/kaggle/working/cinematic_gan_model/discriminator2.keras')\n",
    "        print(\"Discriminator weights loaded\")\n",
    "    except:\n",
    "        print(\"Discriminator weights not found (optional for inference)\")\n",
    "    \n",
    "    # Create NEW optimizers with LOWER learning rates\n",
    "    gen_optimizer = keras.optimizers.Adam(learning_rate=GEN_LR, beta_1=0.0, beta_2=0.9)\n",
    "    disc_optimizer = keras.optimizers.Adam(learning_rate=DISC_LR, beta_1=0.0, beta_2=0.9)\n",
    "    \n",
    "    gen_optimizer = mixed_precision.LossScaleOptimizer(gen_optimizer)\n",
    "    disc_optimizer = mixed_precision.LossScaleOptimizer(disc_optimizer)\n",
    "    \n",
    "    # Create metrics\n",
    "    gen_loss_metric = keras.metrics.Mean(name='gen_loss', dtype=tf.float32)\n",
    "    disc_loss_metric = keras.metrics.Mean(name='disc_loss', dtype=tf.float32)\n",
    "    gan_loss_metric = keras.metrics.Mean(name='gan_loss', dtype=tf.float32)\n",
    "    fm_loss_metric = keras.metrics.Mean(name='fm_loss', dtype=tf.float32)\n",
    "    vgg_loss_metric = keras.metrics.Mean(name='vgg_loss', dtype=tf.float32)\n",
    "    tv_loss_metric = keras.metrics.Mean(name='tv_loss', dtype=tf.float32)\n",
    "\n",
    "print(\"Weights loaded successfully!\")\n",
    "print(f\"New learning rates - Gen: {GEN_LR}, Disc: {DISC_LR}\")\n",
    "\n",
    "print(\"Starting FINE-TUNING...\")\n",
    "\n",
    "\n",
    "train_dataset_dist = strategy.experimental_distribute_dataset(train_dataset)\n",
    "\n",
    "for epoch in range(FINE_TUNE_EPOCHS):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Fine-Tune Epoch {epoch + 1}/{FINE_TUNE_EPOCHS}')\n",
    "    \n",
    "    # Reset metrics\n",
    "    gen_loss_metric.reset_state()\n",
    "    disc_loss_metric.reset_state()\n",
    "    gan_loss_metric.reset_state()\n",
    "    fm_loss_metric.reset_state()\n",
    "    vgg_loss_metric.reset_state()\n",
    "    tv_loss_metric.reset_state()\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(enumerate(train_dataset_dist), total=steps_per_epoch, desc=f'Fine-Tune {epoch + 1}')\n",
    "    \n",
    "    for step, (input_img, target_img) in pbar:\n",
    "        gen_loss, disc_loss, gan_loss, fm_loss, vgg_loss, tv_loss = distributed_train_step(\n",
    "            input_img, target_img\n",
    "        )\n",
    "        gen_loss_metric.update_state(gen_loss)\n",
    "        disc_loss_metric.update_state(disc_loss)\n",
    "        gan_loss_metric.update_state(gan_loss)\n",
    "        fm_loss_metric.update_state(fm_loss)\n",
    "        vgg_loss_metric.update_state(vgg_loss)\n",
    "        tv_loss_metric.update_state(tv_loss)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'G_Loss': f'{gen_loss_metric.result():.4f}',\n",
    "            'D_Loss': f'{disc_loss_metric.result():.4f}',\n",
    "            'TV': f'{tv_loss_metric.result():.4f}',  \n",
    "            'VGG': f'{vgg_loss_metric.result():.4f}'\n",
    "        })\n",
    "\n",
    "    print(f'\\nFine-Tune Epoch {epoch + 1} Summary:')\n",
    "    print(f'  Avg Gen Loss:  {gen_loss_metric.result():.4f}')\n",
    "    print(f'  Avg Disc Loss: {disc_loss_metric.result():.4f}')\n",
    "    print(f'  Avg GAN Loss:  {gan_loss_metric.result():.4f}')\n",
    "    print(f'  Avg FM Loss:   {fm_loss_metric.result():.4f}')\n",
    "    print(f'  Avg VGG Loss:  {vgg_loss_metric.result():.4f}')\n",
    "    print(f'  Avg TV Loss:   {tv_loss_metric.result():.4f} ')\n",
    "    \n",
    "    # Generate and save sample images every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        generate_and_save_images(generator, test_inputs, test_targets, epoch + 1)\n",
    "        print(f'Saved fine-tuned sample images for epoch {epoch + 1}')\n",
    "\n",
    "# Save with new name\n",
    "finetuned_dir = 'cinematic_gan_model_finetuned'\n",
    "os.makedirs(finetuned_dir, exist_ok=True)\n",
    "\n",
    "generator.save(os.path.join(finetuned_dir, 'generator.keras'))\n",
    "print(f'Fine-tuned model saved to: {finetuned_dir}/')\n",
    "print(\"\\nFine-tuning complete!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpuV5e8",
   "dataSources": [
    {
     "datasetId": 4077035,
     "sourceId": 7077993,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31194,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
